{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDA by Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adds a Gradient Reversal Layer and a Discriminator (Classification Head) on top of YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Gradient Reversal Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Gradient Reversal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_metric_learning.utils import common_functions as pml_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversal(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the gradient reversal layer described in\n",
    "    [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818),\n",
    "    which 'leaves the input unchanged during forward propagation\n",
    "    and reverses the gradient by multiplying it\n",
    "    by a negative scalar during backpropagation.'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            weight: The gradients  will be multiplied by ```-alpha```\n",
    "                during the backward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"alpha\", torch.tensor([alpha]))\n",
    "        pml_cf.add_to_recordable_attributes(self, \"alpha\")\n",
    "\n",
    "    def update_weight(self, new_alpha):\n",
    "        self.weight[0] = new_alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return _GradientReversal.apply(x, pml_cf.to_device(self.alpha, x))\n",
    "\n",
    "    def extra_repr(self, delimiter=\"\\n\"):\n",
    "        \"\"\"\"\"\"\n",
    "        return delimiter.join([f\"{a}=str{getattr(self, a)}\" for a in [\"alpha\"]])\n",
    "        # return c_f.extra_repr(self, [\"weight\"])\n",
    "\n",
    "\n",
    "class _GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.alpha * grad_output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([[ 0.1103, -0.2732,  0.1019, -0.1795, -0.0036]])\n",
      "reversed: tensor([[-0.1103,  0.2732, -0.1019,  0.1795,  0.0036]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([0.2084])\n",
      "reversed: tensor([-0.2084])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([[-0.3185]])\n",
      "reversed: tensor([[0.3185]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([0.3650])\n",
      "reversed: tensor([-0.3650])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def test_grad_reverese(alpha):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    input, output = torch.randn(8, 5), torch.randn(8, 1)\n",
    "\n",
    "    network = nn.Sequential(nn.Linear(5, 1), torch.nn.Linear(1, 1))\n",
    "    revnetwork = nn.Sequential(\n",
    "        copy.deepcopy(network), \n",
    "        GradientReversal(alpha),\n",
    "        )\n",
    "\n",
    "    criterion(network(input), output).backward()\n",
    "    criterion(revnetwork(input), output).backward()\n",
    "\n",
    "    for p1, p2 in zip(network.parameters(), revnetwork.parameters()):\n",
    "      print(\"-\"*100)\n",
    "      print(f\"original: {p1.grad}\")\n",
    "      print(f\"reversed: {p2.grad/alpha}\")\n",
    "      assert torch.isclose(p1.grad, ((-p2.grad)/alpha)).all()\n",
    "    print(\"-\"*100)\n",
    "\n",
    "test_grad_reverese(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Initialize YOLOv3 baseline architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "from itertools import chain\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorchyolo.utils.parse_config import parse_model_config\n",
    "from pytorchyolo.utils.utils import weights_init_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modules(module_defs: List[dict]) -> Tuple[dict, nn.ModuleList]:\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "\n",
    "    :param module_defs: List of dictionaries with module definitions\n",
    "    :return: Hyperparameters and pytorch module list\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    hyperparams.update({\n",
    "        'batch': int(hyperparams['batch']),\n",
    "        'subdivisions': int(hyperparams['subdivisions']),\n",
    "        'width': int(hyperparams['width']),\n",
    "        'height': int(hyperparams['height']),\n",
    "        'channels': int(hyperparams['channels']),\n",
    "        'optimizer': hyperparams.get('optimizer'),\n",
    "        'momentum': float(hyperparams['momentum']),\n",
    "        'decay': float(hyperparams['decay']),\n",
    "        'learning_rate': float(hyperparams['learning_rate']),\n",
    "        'burn_in': int(hyperparams['burn_in']),\n",
    "        'max_batches': int(hyperparams['max_batches']),\n",
    "        'policy': hyperparams['policy'],\n",
    "        'lr_steps': list(zip(map(int,   hyperparams[\"steps\"].split(\",\")),\n",
    "                             map(float, hyperparams[\"scales\"].split(\",\"))))\n",
    "    })\n",
    "    assert hyperparams[\"height\"] == hyperparams[\"width\"], \\\n",
    "        \"Height and width should be equal! Non square images are padded with zeros.\"\n",
    "    output_filters = [hyperparams[\"channels\"]]\n",
    "    module_list = nn.ModuleList()\n",
    "    for module_i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if module_def[\"type\"] == \"convolutional\":\n",
    "            bn = int(module_def[\"batch_normalize\"])\n",
    "            filters = int(module_def[\"filters\"])\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            pad = (kernel_size - 1) // 2\n",
    "            modules.add_module(\n",
    "                f\"conv_{module_i}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=output_filters[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=int(module_def[\"stride\"]),\n",
    "                    padding=pad,\n",
    "                    bias=not bn,\n",
    "                ),\n",
    "            )\n",
    "            if bn:\n",
    "                modules.add_module(f\"batch_norm_{module_i}\",\n",
    "                                   nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))\n",
    "            if module_def[\"activation\"] == \"leaky\":\n",
    "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
    "            elif module_def[\"activation\"] == \"mish\":\n",
    "                modules.add_module(f\"mish_{module_i}\", nn.Mish())\n",
    "            elif module_def[\"activation\"] == \"logistic\":\n",
    "                modules.add_module(f\"sigmoid_{module_i}\", nn.Sigmoid())\n",
    "            elif module_def[\"activation\"] == \"swish\":\n",
    "                modules.add_module(f\"swish_{module_i}\", nn.SiLU())\n",
    "\n",
    "        elif module_def[\"type\"] == \"maxpool\":\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            stride = int(module_def[\"stride\"])\n",
    "            if kernel_size == 2 and stride == 1:\n",
    "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride,\n",
    "                                   padding=int((kernel_size - 1) // 2))\n",
    "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
    "\n",
    "        elif module_def[\"type\"] == \"upsample\":\n",
    "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
    "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
    "\n",
    "        elif module_def[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "            filters = sum([output_filters[1:][i] for i in layers]) // int(module_def.get(\"groups\", 1))\n",
    "            modules.add_module(f\"route_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"shortcut\":\n",
    "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
    "            modules.add_module(f\"shortcut_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
    "            # Extract anchors\n",
    "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            num_classes = int(module_def[\"classes\"])\n",
    "            new_coords = bool(module_def.get(\"new_coords\", False))\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, new_coords)\n",
    "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor, mode: str = \"nearest\"):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "\n",
    "    def __init__(self, anchors: List[Tuple[int, int]], num_classes: int, new_coords: bool):\n",
    "        \"\"\"\n",
    "        Create a YOLO layer\n",
    "\n",
    "        :param anchors: List of anchors\n",
    "        :param num_classes: Number of classes\n",
    "        :param new_coords: Whether to use the new coordinate format from YOLO V7\n",
    "        \"\"\"\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.new_coords = new_coords\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.no = num_classes + 5  # number of outputs per anchor\n",
    "        self.grid = torch.zeros(1)  # TODO\n",
    "\n",
    "        anchors = torch.tensor(list(chain(*anchors))).float().view(-1, 2)\n",
    "        self.register_buffer('anchors', anchors)\n",
    "        self.register_buffer(\n",
    "            'anchor_grid', anchors.clone().view(1, -1, 1, 1, 2))\n",
    "        self.stride = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the YOLO layer\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :param img_size: Size of the input image\n",
    "        \"\"\"\n",
    "        stride = img_size // x.size(2)\n",
    "        self.stride = stride\n",
    "        bs, _, ny, nx = x.shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n",
    "        x = x.view(bs, self.num_anchors, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        if not self.training:  # inference\n",
    "            if self.grid.shape[2:4] != x.shape[2:4]:\n",
    "                self.grid = self._make_grid(nx, ny).to(x.device)\n",
    "\n",
    "            if self.new_coords:\n",
    "                x[..., 0:2] = (x[..., 0:2] + self.grid) * stride  # xy\n",
    "                x[..., 2:4] = x[..., 2:4] ** 2 * (4 * self.anchor_grid) # wh\n",
    "            else:\n",
    "                x[..., 0:2] = (x[..., 0:2].sigmoid() + self.grid) * stride  # xy\n",
    "                x[..., 2:4] = torch.exp(x[..., 2:4]) * self.anchor_grid # wh\n",
    "                x[..., 4:] = x[..., 4:].sigmoid() # conf, cls\n",
    "            x = x.view(bs, -1, self.no)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_grid(nx: int = 20, ny: int = 20) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a grid of (x, y) coordinates\n",
    "\n",
    "        :param nx: Number of x coordinates\n",
    "        :param ny: Number of y coordinates\n",
    "        \"\"\"\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')\n",
    "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.yolo_layers = [layer[0]\n",
    "                            for layer in self.module_list if isinstance(layer[0], YOLOLayer)]\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = [] # save feature maps for discriminator\n",
    "        img_size = x.size(2)\n",
    "        layer_outputs, yolo_outputs = [], []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = module(x)\n",
    "            elif module_def[\"type\"] == \"route\":\n",
    "                combined_outputs = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
    "                group_size = combined_outputs.shape[1] // int(module_def.get(\"groups\", 1))\n",
    "                group_id = int(module_def.get(\"group_id\", 0))\n",
    "                x = combined_outputs[:, group_size * group_id : group_size * (group_id + 1)] # Slice groupings used by yolo v4\n",
    "            elif module_def[\"type\"] == \"shortcut\":\n",
    "                layer_i = int(module_def[\"from\"])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def[\"type\"] == \"yolo\":\n",
    "                x = module[0](x, img_size)\n",
    "                yolo_outputs.append(x)\n",
    "            layer_outputs.append(x)\n",
    "            \n",
    "            if i in [81, 93, 105]: # i starts at 0\n",
    "                feature_maps.append(x)\n",
    "        return yolo_outputs, feature_maps if self.training else torch.cat(yolo_outputs, 1)\n",
    "\n",
    "    def load_darknet_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        # Open the weights file\n",
    "        with open(weights_path, \"rb\") as f:\n",
    "            # First five are header values\n",
    "            header = np.fromfile(f, dtype=np.int32, count=5)\n",
    "            self.header_info = header  # Needed to write header when saving weights\n",
    "            self.seen = header[3]  # number of images seen during training\n",
    "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
    "\n",
    "        # Establish cutoff for loading backbone weights\n",
    "        cutoff = None\n",
    "        # If the weights file has a cutoff, we can find out about it by looking at the filename\n",
    "        # examples: darknet53.conv.74 -> cutoff is 74\n",
    "        filename = os.path.basename(weights_path)\n",
    "        if \".conv.\" in filename:\n",
    "            try:\n",
    "                cutoff = int(filename.split(\".\")[-1])  # use last part of filename\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if i == cutoff:\n",
    "                break\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(\n",
    "                    weights[ptr: ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w\n",
    "\n",
    "    def save_darknet_weights(self, path, cutoff=-1):\n",
    "        \"\"\"\n",
    "            @:param path    - path of the new weights file\n",
    "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
    "        \"\"\"\n",
    "        fp = open(path, \"wb\")\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(fp)\n",
    "\n",
    "        # Iterate through layers\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                # If batch norm, load bn first\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    bn_layer = module[1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv bias\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv weights\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Initialize Driscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3-layer MLP for domain classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size=52, h=2048, out_size=1, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_size: size of the input\n",
    "            h: hidden layer size\n",
    "            out_size: size of the output\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.net = nn.Sequential(\n",
    "            GradientReversal(alpha=alpha),\n",
    "            nn.Linear(in_size, h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h, out_size),\n",
    "        )\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.net(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Load model, create dataloader and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from pytorchyolo.utils.datasets import ListDataset\n",
    "from pytorchyolo.utils.augmentations import AUGMENTATION_TRANSFORMS\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorchyolo.test import _evaluate, _create_validation_data_loader\n",
    "from pytorchyolo.utils.loss import compute_loss\n",
    "from pytorchyolo.utils.utils import to_cpu, load_classes, print_environment_info, provide_determinism, worker_seed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CFG = \"/Users/earlranario/Documents/GEMINI/scripts/CropGAN/yolo_uda/configs/yolov3.cfg\"\n",
    "PATH_TO_PRETRAINED = \"/Users/earlranario/Documents/GEMINI/scripts/yolo_grl_data/weights/yolov3.weights\"\n",
    "PATH_TO_TRAIN = \"/Users/earlranario/Documents/GEMINI/scripts/yolo_grl_data/BordenNight/source/train/train.txt\"\n",
    "PATH_TO_VAL = \"/Users/earlranario/Documents/GEMINI/scripts/yolo_grl_data/BordenNight/source/val/val.txt\"\n",
    "PATH_TO_TARGET = \"/Users/earlranario/Documents/GEMINI/scripts/yolo_grl_data/BordenNight/target/images\"\n",
    "EPOCHS = 300\n",
    "N_CPU = 6\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "EVALUATE_INTERVAL = 1\n",
    "IOU_THRESH = 0.5\n",
    "CONF_THRESH = 0.1\n",
    "NMS_THRESH = 0.5\n",
    "ALPHA = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, weights_path=None):\n",
    "    \"\"\"Loads the yolo model from file.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :return: Returns model\n",
    "    :rtype: Darknet\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                          else \"cpu\")  # Select device for inference\n",
    "    model = Darknet(model_path).to(device)\n",
    "\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    # If pretrained weights are specified, start from checkpoint or weight file\n",
    "    if weights_path:\n",
    "        if weights_path.endswith(\".pth\"):\n",
    "            # Load checkpoint weights\n",
    "            model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "        else:\n",
    "            # Load darknet weights\n",
    "            model.load_darknet_weights(weights_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_data_loader(img_path, batch_size, img_size, n_cpu, multiscale_training=False):\n",
    "    \"\"\"Creates a DataLoader for training.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to training images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :param multiscale_training: Scale images to different sizes randomly\n",
    "    :type multiscale_training: bool\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    dataset = ListDataset(\n",
    "        img_path,\n",
    "        img_size=img_size,\n",
    "        multiscale=multiscale_training,\n",
    "        transform=AUGMENTATION_TRANSFORMS)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # num_workers=n_cpu,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        worker_init_fn=worker_seed_set)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = load_model(PATH_TO_CFG, PATH_TO_PRETRAINED)\n",
    "discriminator = Discriminator(alpha=ALPHA)\n",
    "mini_batch_size = model.hyperparams['batch'] // model.hyperparams['subdivisions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataloader = _create_data_loader(\n",
    "    PATH_TO_TRAIN,\n",
    "    mini_batch_size,\n",
    "    model.hyperparams['height'],\n",
    "    N_CPU,\n",
    "    multiscale_training=True)\n",
    "\n",
    "# load validation dataloader\n",
    "validation_dataloader = _create_validation_data_loader(\n",
    "    PATH_TO_VAL,\n",
    "    mini_batch_size,\n",
    "    model.hyperparams['height'],\n",
    "    N_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "if (model.hyperparams['optimizer'] in [None, \"adam\"]):\n",
    "    optimizer = optim.Adam(\n",
    "        params,\n",
    "        lr=model.hyperparams['learning_rate'],\n",
    "        weight_decay=model.hyperparams['decay'],\n",
    "    )\n",
    "elif (model.hyperparams['optimizer'] == \"sgd\"):\n",
    "    optimizer = optim.SGD(\n",
    "        params,\n",
    "        lr=model.hyperparams['learning_rate'],\n",
    "        weight_decay=model.hyperparams['decay'],\n",
    "        momentum=model.hyperparams['momentum'])\n",
    "else:\n",
    "    print(\"Unknown optimizer. Please choose between (adam, sgd).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(\n",
    "      discriminator,\n",
    "      map_features, \n",
    "      labels\n",
    "    ) -> None:\n",
    "\n",
    "    \"\"\" \n",
    "    Discriminator step performed between the source and targer domain. \n",
    "    Input arguments:\n",
    "      map_features: Tensor = feture map obtained from the feature extractor\n",
    "      labels: Tensor = ground truth\n",
    "    Return:\n",
    "      Tensor = cross entropy loss between the prediction and the ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = discriminator(map_features)\n",
    "    discriminator_loss = cross_entropy(outputs, labels)\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target domain set\n",
    "target_imgs_list = os.listdir(PATH_TO_TARGET)\n",
    "t = transforms.Compose([transforms.ToTensor()])\n",
    "target_imgs = [t(Image.open(os.path.join(PATH_TO_TARGET, img))).float() for img in target_imgs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Training Model ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/22 [00:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/0jldz5ws07g7rqlrzt_9ldkr0000gn/T/ipykernel_15852/2812741983.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# run target pass upsample features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mzeros_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mones_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    print(\"\\n---- Training Model ----\")\n",
    "    \n",
    "    model.train() # set yolo model to training mode\n",
    "    discriminator.train() # set discriminator to training mode\n",
    "    random.shuffle(target_imgs)\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=f\"Training Epoch {epoch}\")):\n",
    "        sample = random.sample(range(0, len(target_imgs)), mini_batch_size)\n",
    "        batches_done = len(dataloader) * epoch + batch_i\n",
    "        \n",
    "        source_imgs = imgs.to(device, non_blocking=True)\n",
    "        target_imgs = [target_imgs[i] for i in sample]\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # run source pass, upsample features and calculate yolo loss\n",
    "        source_outputs, source_features = model(source_imgs)\n",
    "        source_features[0] = Upsample(scale_factor=4, mode=\"nearest\")\n",
    "        source_features[1] = Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        yolo_loss, loss_components = compute_loss(source_outputs, targets, model)\n",
    "        \n",
    "        # run target pass upsample features\n",
    "        zeros_label = torch.zeros(mini_batch_size, dtype=torch.long, device=self.device)\n",
    "        ones_label = torch.ones(mini_batch_size, dtype=torch.long, device=self.device)\n",
    "        target_outputs, target_features = model(target_imgs)\n",
    "        target_features[0] = Upsample(scale_factor=4, model=\"nearest\")\n",
    "        target_features[1] = Upsample(scale_factor=2, model=\"nearest\")\n",
    "        \n",
    "        # discriminator step and calculate discriminator loss\n",
    "        discriminator_source_loss = discriminator_step(discriminator, source_features, zeros_label)\n",
    "        discriminator_target_loss = discriminator_step(discriminator, target_features, ones_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_grl",
   "language": "python",
   "name": "yolo_grl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
