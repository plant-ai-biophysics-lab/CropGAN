{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDA by Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adds a Gradient Reversal Layer and a Discriminator (Classification Head) on top of YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Gradient Reversal Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Gradient Reversal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eranario/miniconda3/envs/yolo-uda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_metric_learning.utils import common_functions as pml_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversal(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the gradient reversal layer described in\n",
    "    [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818),\n",
    "    which 'leaves the input unchanged during forward propagation\n",
    "    and reverses the gradient by multiplying it\n",
    "    by a negative scalar during backpropagation.'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            weight: The gradients  will be multiplied by ```-alpha```\n",
    "                during the backward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"alpha\", torch.tensor([alpha]))\n",
    "        pml_cf.add_to_recordable_attributes(self, \"alpha\")\n",
    "\n",
    "    def update_weight(self, new_alpha):\n",
    "        self.weight[0] = new_alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return _GradientReversal.apply(x, pml_cf.to_device(self.alpha, x))\n",
    "\n",
    "    def extra_repr(self, delimiter=\"\\n\"):\n",
    "        \"\"\"\"\"\"\n",
    "        return delimiter.join([f\"{a}=str{getattr(self, a)}\" for a in [\"alpha\"]])\n",
    "        # return c_f.extra_repr(self, [\"weight\"])\n",
    "\n",
    "\n",
    "class _GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.alpha * grad_output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([[ 0.0636, -0.0822,  0.0423, -0.0366,  0.2136]])\n",
      "reversed: tensor([[-0.0636,  0.0822, -0.0423,  0.0366, -0.2136]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([-0.1749])\n",
      "reversed: tensor([0.1749])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([[-0.4986]])\n",
      "reversed: tensor([[0.4986]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "original: tensor([-1.7136])\n",
      "reversed: tensor([1.7136])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def test_grad_reverese(alpha):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    input, output = torch.randn(8, 5), torch.randn(8, 1)\n",
    "\n",
    "    network = nn.Sequential(nn.Linear(5, 1), torch.nn.Linear(1, 1))\n",
    "    revnetwork = nn.Sequential(\n",
    "        copy.deepcopy(network), \n",
    "        GradientReversal(alpha),\n",
    "        )\n",
    "\n",
    "    criterion(network(input), output).backward()\n",
    "    criterion(revnetwork(input), output).backward()\n",
    "\n",
    "    for p1, p2 in zip(network.parameters(), revnetwork.parameters()):\n",
    "      print(\"-\"*100)\n",
    "      print(f\"original: {p1.grad}\")\n",
    "      print(f\"reversed: {p2.grad/alpha}\")\n",
    "      assert torch.isclose(p1.grad, ((-p2.grad)/alpha)).all()\n",
    "    print(\"-\"*100)\n",
    "\n",
    "test_grad_reverese(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Initialize YOLOv3 baseline architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "from itertools import chain\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorchyolo.utils.parse_config import parse_model_config\n",
    "from pytorchyolo.utils.utils import weights_init_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modules(module_defs: List[dict]) -> Tuple[dict, nn.ModuleList]:\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "\n",
    "    :param module_defs: List of dictionaries with module definitions\n",
    "    :return: Hyperparameters and pytorch module list\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    hyperparams.update({\n",
    "        'batch': int(hyperparams['batch']),\n",
    "        'subdivisions': int(hyperparams['subdivisions']),\n",
    "        'width': int(hyperparams['width']),\n",
    "        'height': int(hyperparams['height']),\n",
    "        'channels': int(hyperparams['channels']),\n",
    "        'optimizer': hyperparams.get('optimizer'),\n",
    "        'momentum': float(hyperparams['momentum']),\n",
    "        'decay': float(hyperparams['decay']),\n",
    "        'learning_rate': float(hyperparams['learning_rate']),\n",
    "        'burn_in': int(hyperparams['burn_in']),\n",
    "        'max_batches': int(hyperparams['max_batches']),\n",
    "        'policy': hyperparams['policy'],\n",
    "        'lr_steps': list(zip(map(int,   hyperparams[\"steps\"].split(\",\")),\n",
    "                             map(float, hyperparams[\"scales\"].split(\",\"))))\n",
    "    })\n",
    "    assert hyperparams[\"height\"] == hyperparams[\"width\"], \\\n",
    "        \"Height and width should be equal! Non square images are padded with zeros.\"\n",
    "    output_filters = [hyperparams[\"channels\"]]\n",
    "    module_list = nn.ModuleList()\n",
    "    for module_i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if module_def[\"type\"] == \"convolutional\":\n",
    "            bn = int(module_def[\"batch_normalize\"])\n",
    "            filters = int(module_def[\"filters\"])\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            pad = (kernel_size - 1) // 2\n",
    "            modules.add_module(\n",
    "                f\"conv_{module_i}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=output_filters[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=int(module_def[\"stride\"]),\n",
    "                    padding=pad,\n",
    "                    bias=not bn,\n",
    "                ),\n",
    "            )\n",
    "            if bn:\n",
    "                modules.add_module(f\"batch_norm_{module_i}\",\n",
    "                                   nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))\n",
    "            if module_def[\"activation\"] == \"leaky\":\n",
    "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
    "            elif module_def[\"activation\"] == \"mish\":\n",
    "                modules.add_module(f\"mish_{module_i}\", nn.Mish())\n",
    "            elif module_def[\"activation\"] == \"logistic\":\n",
    "                modules.add_module(f\"sigmoid_{module_i}\", nn.Sigmoid())\n",
    "            elif module_def[\"activation\"] == \"swish\":\n",
    "                modules.add_module(f\"swish_{module_i}\", nn.SiLU())\n",
    "\n",
    "        elif module_def[\"type\"] == \"maxpool\":\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            stride = int(module_def[\"stride\"])\n",
    "            if kernel_size == 2 and stride == 1:\n",
    "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride,\n",
    "                                   padding=int((kernel_size - 1) // 2))\n",
    "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
    "\n",
    "        elif module_def[\"type\"] == \"upsample\":\n",
    "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
    "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
    "\n",
    "        elif module_def[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "            filters = sum([output_filters[1:][i] for i in layers]) // int(module_def.get(\"groups\", 1))\n",
    "            modules.add_module(f\"route_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"shortcut\":\n",
    "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
    "            modules.add_module(f\"shortcut_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
    "            # Extract anchors\n",
    "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            num_classes = int(module_def[\"classes\"])\n",
    "            new_coords = bool(module_def.get(\"new_coords\", False))\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, new_coords)\n",
    "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor, mode: str = \"nearest\"):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "\n",
    "    def __init__(self, anchors: List[Tuple[int, int]], num_classes: int, new_coords: bool):\n",
    "        \"\"\"\n",
    "        Create a YOLO layer\n",
    "\n",
    "        :param anchors: List of anchors\n",
    "        :param num_classes: Number of classes\n",
    "        :param new_coords: Whether to use the new coordinate format from YOLO V7\n",
    "        \"\"\"\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.new_coords = new_coords\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.no = num_classes + 5  # number of outputs per anchor\n",
    "        self.grid = torch.zeros(1)  # TODO\n",
    "\n",
    "        anchors = torch.tensor(list(chain(*anchors))).float().view(-1, 2)\n",
    "        self.register_buffer('anchors', anchors)\n",
    "        self.register_buffer(\n",
    "            'anchor_grid', anchors.clone().view(1, -1, 1, 1, 2))\n",
    "        self.stride = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the YOLO layer\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :param img_size: Size of the input image\n",
    "        \"\"\"\n",
    "        stride = img_size // x.size(2)\n",
    "        self.stride = stride\n",
    "        bs, _, ny, nx = x.shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n",
    "        x = x.view(bs, self.num_anchors, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        if not self.training:  # inference\n",
    "            if self.grid.shape[2:4] != x.shape[2:4]:\n",
    "                self.grid = self._make_grid(nx, ny).to(x.device)\n",
    "\n",
    "            if self.new_coords:\n",
    "                x[..., 0:2] = (x[..., 0:2] + self.grid) * stride  # xy\n",
    "                x[..., 2:4] = x[..., 2:4] ** 2 * (4 * self.anchor_grid) # wh\n",
    "            else:\n",
    "                x[..., 0:2] = (x[..., 0:2].sigmoid() + self.grid) * stride  # xy\n",
    "                x[..., 2:4] = torch.exp(x[..., 2:4]) * self.anchor_grid # wh\n",
    "                x[..., 4:] = x[..., 4:].sigmoid() # conf, cls\n",
    "            x = x.view(bs, -1, self.no)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_grid(nx: int = 20, ny: int = 20) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a grid of (x, y) coordinates\n",
    "\n",
    "        :param nx: Number of x coordinates\n",
    "        :param ny: Number of y coordinates\n",
    "        \"\"\"\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')\n",
    "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.module_defs = parse_model_config(config_path)\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.yolo_layers = [layer[0]\n",
    "                            for layer in self.module_list if isinstance(layer[0], YOLOLayer)]\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = [] # save feature maps for discriminator\n",
    "        img_size = x.size(2)\n",
    "        layer_outputs, yolo_outputs = [], []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = module(x)\n",
    "            elif module_def[\"type\"] == \"route\":\n",
    "                combined_outputs = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
    "                group_size = combined_outputs.shape[1] // int(module_def.get(\"groups\", 1))\n",
    "                group_id = int(module_def.get(\"group_id\", 0))\n",
    "                x = combined_outputs[:, group_size * group_id : group_size * (group_id + 1)] # Slice groupings used by yolo v4\n",
    "            elif module_def[\"type\"] == \"shortcut\":\n",
    "                layer_i = int(module_def[\"from\"])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def[\"type\"] == \"yolo\":\n",
    "                x = module[0](x, img_size)\n",
    "                yolo_outputs.append(x)\n",
    "            layer_outputs.append(x)\n",
    "            \n",
    "            if i in [81, 93, 105]: # i starts at 0\n",
    "                feature_maps.append(x)\n",
    "        return [yolo_outputs, feature_maps] if self.training else torch.cat(yolo_outputs, 1)\n",
    "\n",
    "    def load_darknet_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        # Open the weights file\n",
    "        with open(weights_path, \"rb\") as f:\n",
    "            # First five are header values\n",
    "            header = np.fromfile(f, dtype=np.int32, count=5)\n",
    "            self.header_info = header  # Needed to write header when saving weights\n",
    "            self.seen = header[3]  # number of images seen during training\n",
    "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
    "\n",
    "        # Establish cutoff for loading backbone weights\n",
    "        cutoff = None\n",
    "        # If the weights file has a cutoff, we can find out about it by looking at the filename\n",
    "        # examples: darknet53.conv.74 -> cutoff is 74\n",
    "        filename = os.path.basename(weights_path)\n",
    "        if \".conv.\" in filename:\n",
    "            try:\n",
    "                cutoff = int(filename.split(\".\")[-1])  # use last part of filename\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if i == cutoff:\n",
    "                break\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(\n",
    "                    weights[ptr: ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w\n",
    "\n",
    "    def save_darknet_weights(self, path, cutoff=-1):\n",
    "        \"\"\"\n",
    "            @:param path    - path of the new weights file\n",
    "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
    "        \"\"\"\n",
    "        fp = open(path, \"wb\")\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(fp)\n",
    "\n",
    "        # Iterate through layers\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                # If batch norm, load bn first\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    bn_layer = module[1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv bias\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv weights\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Initialize Driscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3-layer MLP + Greadient Reversal Layer for domain classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size=52, h=2048, out_size=1, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_size: size of the input\n",
    "            h: hidden layer size\n",
    "            out_size: size of the output\n",
    "            alpha: grl constant\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.net = nn.Sequential(\n",
    "            GradientReversal(alpha=alpha),\n",
    "            nn.Linear(in_size, h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h, out_size),\n",
    "        )\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.net(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Load model, create dataloader and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pytorchyolo.utils.datasets import ListDataset\n",
    "from pytorchyolo.utils.augmentations import AUGMENTATION_TRANSFORMS\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorchyolo.test import _create_validation_data_loader\n",
    "from pytorchyolo.utils.loss import compute_loss\n",
    "from pytorchyolo.utils.utils import to_cpu, worker_seed_set, ap_per_class, get_batch_statistics, non_max_suppression, xywh2xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CFG = \"/group/jmearlesgrp/scratch/eranario/CropGAN/yolo_uda/configs/yolov3.cfg\"\n",
    "PATH_TO_PRETRAINED = \"/group/jmearlesgrp/data/yolo_grl_data/weights/yolov3.weights\"\n",
    "PATH_TO_TRAIN = \"/group/jmearlesgrp/data/yolo_grl_data/BordenNight/source/train/train.txt\"\n",
    "# PATH_TO_VAL = \"/group/jmearlesgrp/data/yolo_grl_data/BordenNight/source/val/val.txt\"\n",
    "PATH_TO_VAL = '/group/jmearlesgrp/data/yolo_grl_data/BordenNight/target/target.txt'\n",
    "PATH_TO_TARGET = \"/group/jmearlesgrp/data/yolo_grl_data/BordenNight/target/images\"\n",
    "EPOCHS = 300\n",
    "N_CPU = 6\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "EVALUATE_INTERVAL = 1\n",
    "IOU_THRESH = 0.5\n",
    "CONF_THRESH = 0.1\n",
    "NMS_THRESH = 0.5\n",
    "alpha = 1.0\n",
    "VERBOSE = False\n",
    "CLASS_NAMES = [\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, weights_path=None):\n",
    "    \"\"\"Loads the yolo model from file.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :return: Returns model\n",
    "    :rtype: Darknet\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                          else \"cpu\")  # Select device for inference\n",
    "    model = Darknet(model_path).to(device)\n",
    "\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    # If pretrained weights are specified, start from checkpoint or weight file\n",
    "    if weights_path:\n",
    "        if weights_path.endswith(\".pth\"):\n",
    "            # Load checkpoint weights\n",
    "            model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "        else:\n",
    "            # Load darknet weights\n",
    "            model.load_darknet_weights(weights_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_data_loader(img_path, batch_size, img_size, n_cpu, multiscale_training=False):\n",
    "    \"\"\"Creates a DataLoader for training.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to training images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :param multiscale_training: Scale images to different sizes randomly\n",
    "    :type multiscale_training: bool\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    dataset = ListDataset(\n",
    "        img_path,\n",
    "        img_size=img_size,\n",
    "        multiscale=multiscale_training,\n",
    "        transform=AUGMENTATION_TRANSFORMS)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_cpu,\n",
    "        # num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        worker_init_fn=worker_seed_set)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = load_model(PATH_TO_CFG, PATH_TO_PRETRAINED)\n",
    "discriminator = Discriminator(alpha=alpha)\n",
    "mini_batch_size = model.hyperparams['batch'] // model.hyperparams['subdivisions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataloader = _create_data_loader(\n",
    "    PATH_TO_TRAIN,\n",
    "    mini_batch_size,\n",
    "    model.hyperparams['height'],\n",
    "    N_CPU,\n",
    "    multiscale_training=False)\n",
    "\n",
    "# load validation dataloader\n",
    "validation_dataloader = _create_validation_data_loader(\n",
    "    PATH_TO_VAL,\n",
    "    mini_batch_size,\n",
    "    model.hyperparams['height'],\n",
    "    N_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "if (model.hyperparams['optimizer'] in [None, \"adam\"]):\n",
    "    optimizer = optim.Adam(\n",
    "        params,\n",
    "        lr=model.hyperparams['learning_rate'],\n",
    "        weight_decay=model.hyperparams['decay'],\n",
    "    )\n",
    "elif (model.hyperparams['optimizer'] == \"sgd\"):\n",
    "    optimizer = optim.SGD(\n",
    "        params,\n",
    "        lr=model.hyperparams['learning_rate'],\n",
    "        weight_decay=model.hyperparams['decay'],\n",
    "        momentum=model.hyperparams['momentum'])\n",
    "else:\n",
    "    print(\"Unknown optimizer. Please choose between (adam, sgd).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import wandb\n",
    "from terminaltables import AsciiTable\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_stats(metrics_output, class_names, verbose):\n",
    "    if metrics_output is not None:\n",
    "        precision, recall, AP, f1, ap_class = metrics_output\n",
    "        if verbose:\n",
    "            # Prints class AP and mean AP\n",
    "            ap_table = [[\"Index\", \"Class\", \"AP\"]]\n",
    "            for i, c in enumerate(ap_class):\n",
    "                ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
    "            print(AsciiTable(ap_table).table)\n",
    "        print(f\"---- mAP {AP.mean():.5f} ----\")\n",
    "    else:\n",
    "        print(\"---- mAP not measured (no detections found by model) ----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(model, dataloader, class_names, img_size, iou_thres, conf_thres, nms_thres, verbose):\n",
    "    \"\"\"Evaluate model on validation dataset.\n",
    "\n",
    "    :param model: Model to evaluate\n",
    "    :type model: models.Darknet\n",
    "    :param dataloader: Dataloader provides the batches of images with targets\n",
    "    :type dataloader: DataLoader\n",
    "    :param class_names: List of class names\n",
    "    :type class_names: [str]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param iou_thres: IOU threshold required to qualify as detected\n",
    "    :type iou_thres: float\n",
    "    :param conf_thres: Object confidence threshold\n",
    "    :type conf_thres: float\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression\n",
    "    :type nms_thres: float\n",
    "    :param verbose: If True, prints stats of model\n",
    "    :type verbose: bool\n",
    "    :return: Returns precision, recall, AP, f1, ap_class\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(f\"model training: {model.training}\")\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    labels = []\n",
    "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
    "    for _, imgs, targets in tqdm.tqdm(dataloader, desc=\"Validating\"):\n",
    "        # Extract labels\n",
    "        labels += targets[:, 1].tolist()\n",
    "        # Rescale target\n",
    "        targets[:, 2:] = xywh2xyxy(targets[:, 2:])\n",
    "        targets[:, 2:] *= img_size\n",
    "\n",
    "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            outputs = non_max_suppression(outputs, conf_thres=conf_thres, iou_thres=nms_thres)\n",
    "\n",
    "        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n",
    "\n",
    "    if len(sample_metrics) == 0:  # No detections over whole validation set.\n",
    "        print(\"---- No detections over whole validation set ----\")\n",
    "        return None\n",
    "\n",
    "    # Concatenate sample statistics\n",
    "    true_positives, pred_scores, pred_labels = [\n",
    "        np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "    metrics_output = ap_per_class(\n",
    "        true_positives, pred_scores, pred_labels, labels)\n",
    "\n",
    "    print_eval_stats(metrics_output, class_names, verbose)\n",
    "\n",
    "    return metrics_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(\n",
    "      discriminator,\n",
    "      map_features, \n",
    "      labels\n",
    "    ) -> None:\n",
    "\n",
    "    \"\"\" \n",
    "    Discriminator step performed between the source and targer domain. \n",
    "    Input arguments:\n",
    "      map_features: Tensor = feture map obtained from the feature extractor\n",
    "      labels: Tensor = ground truth\n",
    "    Return:\n",
    "      Tensor = cross entropy loss between the prediction and the ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = discriminator(map_features)\n",
    "    outputs = outputs.view(mini_batch_size, -1)\n",
    "    discriminator_loss = cross_entropy(outputs, labels)\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): GradientReversal(alpha=strtensor([1.], device='cuda:0'))\n",
       "    (1): Linear(in_features=52, out_features=2048, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move models to GPU\n",
    "model.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target domain set\n",
    "target_imgs_list = os.listdir(PATH_TO_TARGET)\n",
    "t = transforms.Compose([\n",
    "    transforms.Resize((416,416)),\n",
    "    transforms.ToTensor()])\n",
    "target_imgs = [t(Image.open(os.path.join(PATH_TO_TARGET, img))).float() for img in target_imgs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mearlranario\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/group/jmearlesgrp/scratch/eranario/CropGAN/yolo_uda/wandb/run-20230918_155515-28dcgp23</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/earlranario/yolo-uda/runs/28dcgp23' target=\"_blank\">smooth-water-23</a></strong> to <a href='https://wandb.ai/earlranario/yolo-uda' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/earlranario/yolo-uda' target=\"_blank\">https://wandb.ai/earlranario/yolo-uda</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/earlranario/yolo-uda/runs/28dcgp23' target=\"_blank\">https://wandb.ai/earlranario/yolo-uda/runs/28dcgp23</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize wandb\n",
    "run = wandb.init(project='yolo-uda', notes='initial config')\n",
    "wandb.config = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"iou_thresh\": IOU_THRESH,\n",
    "    \"conf_thresh\": CONF_THRESH,\n",
    "    \"nms_thresh\": NMS_THRESH,\n",
    "    \"alpha\": alpha,\n",
    "    \"img_size\": 416,\n",
    "    \"momentum\": 0.9,\n",
    "    \"decay\": 0.0005,\n",
    "    \"angle\": 0,\n",
    "    \"saturation\": 1.5,\n",
    "    \"exposure\": 1.5,\n",
    "    \"hue\": 0.1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"burn_in\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Training Model ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 87/87 [01:25<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating Model ----\n",
      "model training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 38/38 [00:02<00:00, 15.72it/s]\n",
      "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 1854.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- mAP 0.00000 ----\n",
      "\n",
      "---- Training Model ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  10%|█         | 9/87 [00:11<01:36,  1.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3756892/802138992.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0myolo_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# run optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/yolo-uda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/yolo-uda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "upsample_4 = Upsample(scale_factor=4, mode=\"nearest\")\n",
    "upsample_2 = Upsample(scale_factor=2, mode=\"nearest\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    print(\"\\n---- Training Model ----\")\n",
    "\n",
    "    # set to training mode\n",
    "    model.train() # set yolo model to training mode\n",
    "    discriminator.train() # set discriminator to training mode\n",
    "    \n",
    "    random.shuffle(target_imgs)\n",
    "    \n",
    "    for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=f\"Training Epoch {epoch}\")):\n",
    "        sample = random.sample(range(0, len(target_imgs)), mini_batch_size)\n",
    "        batches_done = len(dataloader) * epoch + batch_i\n",
    "        \n",
    "        source_imgs = imgs.to(device)\n",
    "        target_imgs = [target_imgs[i] for i in sample]\n",
    "        target_imgs = torch.stack(target_imgs, dim=0).to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # run source pass, upsample features and calculate yolo loss\n",
    "        source_outputs, source_features = model(source_imgs)\n",
    "        source_features[0] = upsample_4(source_features[0])\n",
    "        source_features[1] = upsample_2(source_features[1])\n",
    "        yolo_loss, loss_components = compute_loss(source_outputs, targets, model)\n",
    "        \n",
    "        # run target pass upsample features\n",
    "        zeros_label = torch.zeros(mini_batch_size, dtype=torch.long, device=device)\n",
    "        ones_label = torch.ones(mini_batch_size, dtype=torch.long, device=device)\n",
    "        target_outputs, target_features = model(target_imgs)\n",
    "        target_features[0] = upsample_4(target_features[0])\n",
    "        target_features[1] = upsample_2(target_features[1])\n",
    "        \n",
    "        # concatenate source and target features\n",
    "        source_features = torch.cat(source_features, dim=1).to(device)\n",
    "        target_features = torch.cat(target_features, dim=1).to(device)\n",
    "        \n",
    "        # discriminator step and calculate discriminator loss\n",
    "        discriminator_source_loss = discriminator_step(discriminator, source_features, zeros_label)\n",
    "        discriminator_target_loss = discriminator_step(discriminator, target_features, ones_label)\n",
    "        discriminator_loss = discriminator_source_loss + discriminator_target_loss\n",
    "\n",
    "        yolo_loss.backward(retain_graph=True) \n",
    "        discriminator_loss.backward()\n",
    "        \n",
    "        # run optimizer\n",
    "        if batches_done % model.hyperparams['subdivisions'] == 0:\n",
    "            # adapt learning rate\n",
    "            lr = model.hyperparams['learning_rate']\n",
    "            if batches_done < model.hyperparams['burn_in']:\n",
    "                lr *= (batches_done / model.hyperparams['burn_in'])\n",
    "            else:\n",
    "                for threshold, value in model.hyperparams['lr_steps']:\n",
    "                    if batches_done > threshold:\n",
    "                        lr *= value\n",
    "            ## log the learning rate here ##\n",
    "            wandb.log({\"lr\": lr})\n",
    "            # set leraning rate\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "                \n",
    "            # Run optimizer\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "        ## log progress here ##\n",
    "        if VERBOSE:\n",
    "            print(AsciiTable(\n",
    "                    [\n",
    "                        [\"Type\", \"Value\"],\n",
    "                        [\"IoU loss\", float(loss_components[0])],\n",
    "                        [\"Object loss\", float(loss_components[1])],\n",
    "                        [\"Class loss\", float(loss_components[2])],\n",
    "                        [\"Loss\", float(loss_components[3])],\n",
    "                        [\"Source loss\", float(discriminator_source_loss)],\n",
    "                        [\"Target loss\", float(discriminator_target_loss)],\n",
    "                        [\"Batch loss\", to_cpu(loss).item()]\n",
    "                    ]).table)\n",
    "        wandb.log({\n",
    "            \"iou_loss\": float(loss_components[0]),\n",
    "            \"obj_loss\": float(loss_components[1]),\n",
    "            \"cls_loss\": float(loss_components[2]),\n",
    "            \"loss\": float(loss_components[3]),\n",
    "            \"src_loss\": float(discriminator_source_loss),\n",
    "            \"trgt_loss\": float(discriminator_target_loss)\n",
    "            })\n",
    "        model.seen += imgs.size(0)\n",
    "        \n",
    "    # save model to checkpoint file\n",
    "    # if epoch % args.checkpoint_interval == 0:\n",
    "    #     checkpoint_path = f\"checkpoints/yolov3_ckpt_{epoch}.pth\"\n",
    "    #     print(f\"---- Saving checkpoint to: '{checkpoint_path}' ----\")\n",
    "    #     torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    # evaluate\n",
    "    if epoch % EVALUATE_INTERVAL == 0:\n",
    "        print(\"\\n---- Evaluating Model ----\")\n",
    "        # Evaluate the model on the validation set\n",
    "        metrics_output = _evaluate(\n",
    "            model,\n",
    "            validation_dataloader,\n",
    "            CLASS_NAMES,\n",
    "            img_size=model.hyperparams['height'],\n",
    "            iou_thres=IOU_THRESH,\n",
    "            conf_thres=CONF_THRESH,\n",
    "            nms_thres=NMS_THRESH,\n",
    "            verbose=VERBOSE\n",
    "        )\n",
    "\n",
    "        if metrics_output is not None:\n",
    "            precision, recall, AP, f1, ap_class = metrics_output\n",
    "            wandb.log({\n",
    "                \"precision\": precision.mean(),\n",
    "                \"recall\": recall.mean(),\n",
    "                \"f1\": f1.mean(),\n",
    "                \"mAP\": AP.mean()\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model locally\n",
    "SAVE = '/group/jmearlesgrp/data/yolo_grl_data/weights/baseline/20230914_yolov3_uda.pth'\n",
    "torch.save(model.state_dict(), SAVE)\n",
    "\n",
    "# save model to wandb\n",
    "best_model = wandb.Artifact(f\"20230914_yolov3_uda_{run.id}\", type=\"model\")\n",
    "best_model.add_file(\"/group/jmearlesgrp/data/yolo_grl_data/weights/baseline/20230914_yolov3_uda.pth\")\n",
    "run.log_artifact(best_model)\n",
    "run.link_artifact(best_model, \"model-registry/yolo-uda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-uda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18 (main, Sep 11 2023, 13:41:44) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0260efa9996269a06c1dff44d5dce9e84fc8003d2759b53626bcf02bfd994e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
